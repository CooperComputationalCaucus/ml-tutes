{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 tutorial: Linear regression\n",
    "\n",
    "### Introduction\n",
    "In this exercise, you will implement linear regression and get to see it work on data. Hopefully you have watched the video lectures associated with this week. **Please keep the function declarations as they are.**\n",
    "\n",
    "To get started with this exercise, you will need to download the Week 1 folder from the github or dropbox.\n",
    "\n",
    "### External files included for this exercise\n",
    " - $\\texttt{ex1data1.txt}$ - Dataset for linear regression with one variable\n",
    " - $\\texttt{ex1data2.txt}$ - Dataset for linear regression with multiple variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Simple numpy function\n",
    "\n",
    "Let's kick things off by making a happy little identity matrix. \n",
    "\n",
    "**Exercise:** Make a function that takes nothing in, and returns a 5x5 identity matrix.\n",
    "\n",
    "*Hint: check the documentation for `np.eye()` and `np.identity()`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def warmUp():\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return A\n",
    "print('Diagonal matrix of ones:\\n', warmUp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "`Diagonal matrix of ones:\n",
    "[[1. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0.]\n",
    " [0. 0. 1. 0. 0.]\n",
    " [0. 0. 0. 1. 0.]\n",
    " [0. 0. 0. 0. 1.]]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Linear regression with one variable\n",
    "In this section you will implement linear regression with a single varaible to predict profits. We have ascertained data from food trucks in various cities that breaks down the profits and populations of those cities. A particularly daft business executive from *Mangia con Maffetone LLC* has approached you with this data and asked for advice on which city to expand to next.\n",
    "\n",
    "The file $\\texttt{ex1data1.txt}$ contains the dataset for our linear regression prob- lem. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss. The file is loaded using the commands in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('./data/ex1data1.txt',delimiter=',') #Loads simple comma separated values\n",
    "X = data[:,0] # Sets all (:) of the first (0) column equal to a new variable X\n",
    "m = X.shape[0]\n",
    "y = data[:,1].reshape((m,1)) # Sets the second column to y\n",
    "# This makes y have the shape (97,1) instead of (97,). This is an important implementation distinction. \n",
    "print(\"Number of training examples =\",m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Plotting the data\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population). (Many other problems that you will encounter in data science are multi-dimensional and can’t be plotted on a 2D plot.)\n",
    "\n",
    "**Exercise:** Using the X, y variables from above create a plot of the 2D data. Label the y-axis as \"Profit in 10,000s\" and the x-axis \"Population of City in 10,000s\". Use red x's for the markers.<br>\n",
    "*Hint: matplotlib.pyplot has been imported as plt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<img src=\"data/plot.png\" style=\"width: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Gradient Descent\n",
    "In this part, you will fit the linear regression parameters θ to our dataset using gradient descent.\n",
    "\n",
    "#### 2.2.1 - Update Equations\n",
    "The objective of linear regression is to minimize the cost (loss) function\n",
    "\\begin{equation*}\n",
    "J(\\theta) = \\frac{1}{2m}\\sum^{m}_{i=1} (h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "\\end{equation*}\n",
    "where the hypothesis $h_\\theta(x)$ is given by the linear model\n",
    "\\begin{equation*}\n",
    "h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1x_1.\n",
    "\\end{equation*}\n",
    "\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update\n",
    "\\begin{equation*}\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum^{m}_{i=1} (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n",
    "\\end{equation*}\n",
    "simultaneously for all $j$. \n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\\theta)$.\n",
    "\n",
    "#### 2.2.2 - Implementation\n",
    "- Below the data is already set up for linear regression. We add another dimesion (of ones) to our data to accomodate the $\\theta_0$ intercept term. \n",
    "- We also initialize the parameters to 0 and the learning rate `alpha` to 0.01.\n",
    "- Finally, because of array broadcasting rules, life will be easier to debug if we enforce linear algebra. Thus when you make vectors (nx1 arrays), force them into a (n,1) shape instead of the numpy default of (n,). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((m,2)) #Start with a matrix of zeros with 2 columns of m training examples in each row\n",
    "X[:,0] = np.ones(m) # Set the first column to all 1, to multiply against constant parameter\n",
    "X[:,1] = data[:,0] # X data for the first order linear parameter\n",
    "theta = np.zeros((2,1)) # Initialize fitting parameters (weights)\n",
    "\n",
    "iterations = 1500\n",
    "alpha = 0.01 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 - Computing the cost $J(\\theta)$\n",
    "As you perform gradient descent to learn minimize the cost function $J(\\theta)$, it is helpful to monitor the convergence by computing the cost. In this section, you will implement a function to calculate  $J(\\theta)$ so you can check the convergence of your gradient descent implementation.\n",
    "\n",
    "**Exercise:** Complete the function `compute_cost()`, which is a function that computes --- you guessed it --- $J(\\theta)$. <br>\n",
    "*Hint: As you are doing this, remember that $X$ and $y$ are not scalar values, but matricies whose rows represent the examples from the training set. Consider using `np.matmul()` or `np.dot()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X,y,theta):\n",
    "    '''\n",
    "    Computes the cost for linear regression\n",
    "    J = compute_cost(X,y,theta) computes the cost of using theta as the\n",
    "    parameter for linear regression to fit the data points in X and y.\n",
    "    '''\n",
    "    \n",
    "    m = y.shape[0] #Number of training examples\n",
    "    J = 0;\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return J\n",
    "print(compute_cost(X,y,theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**<br>\n",
    "32.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 - Gradient Descent\n",
    "Next you will implement the gradient descent algorithm. The loop structure has been written for you, and you only need to supply the updates to $\\theta$ within each iteration. \n",
    "\n",
    "As you program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the cost $J(\\theta)$ is parameterized by the vector $\\theta$, and not $X$ and $y$. That is, we minimize the value of $J(\\theta$) by changing the values of the vector $\\theta$, and not by changing $X$ or $y$. \n",
    "\n",
    "**Exercise:** Complete the loop in the function `gradient_descent()` to provide the update for $\\theta$. Please refer to the equations from the video lectures, or above.<br>\n",
    "*Hint: Make sure to vectorize your code using matrix multiplications. If you are unsure of operations, it may help to consider the shape of given arrays and the necessary resultand shapes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,theta,alpha,num_iters):\n",
    "    '''\n",
    "    Performs gradient descent to learn theta\n",
    "    \n",
    "    Inputs\n",
    "    ===============\n",
    "    X: input features\n",
    "    y: desired ouput values\n",
    "    alpha: learning rrate\n",
    "    num_iters: number of iteratons for learning\n",
    "    \n",
    "    Returns\n",
    "    ===============\n",
    "    theta: final learned weights\n",
    "    J_history: list of loss over each itteration\n",
    "    '''\n",
    "    m = y.shape[0] #Number of training examples\n",
    "    J_history = np.zeros((num_iters,1))\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        J_history[i] = compute_cost(X,y,theta)\n",
    "    return theta, J_history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we run the gradient descent function and plot the resultant linear regression fit, as well as the returned loss history. This history allows us to ensure that the loss function is indeed decreasing over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example run\n",
    "learned_theta, J_hist = gradient_descent(X,y,theta,alpha,iterations)\n",
    "# Example plotting\n",
    "fig=plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.plot(X[:,1],y,'rx',MarkerSize=5)\n",
    "plt.xlabel(\"Population of City in 10,000s\")\n",
    "plt.ylabel(\"Profit in $10,000s\")\n",
    "ax1.plot(X[:,1],np.dot(X,learned_theta),'b-')\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.plot([a for a in range(iterations)], J_hist,'k.',MarkerSize=5)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "print('Learned value of theta0:{:7.3f}'.format(learned_theta[0,0]),\n",
    "      '\\nLearned value of theta1:{:7.3f}'.format(learned_theta[1,0]))\n",
    "# Make a prediction\n",
    "predict1 = np.dot(np.array([1,3.5]),learned_theta)\n",
    "print(predict1)\n",
    "print('For population = 35,000, we predict a profit of ${:8.2f}'.format(predict1[0]*10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<img src=\"data/plot2.png\" style=\"width: 600px\"><br>\n",
    "Learned value of theta0: -3.630 <br>\n",
    "Learned value of theta1:  1.166<br>\n",
    "For population = 35,000, we predict a profit of $ 4519.77\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Visualizing $J(\\theta)$\n",
    "\n",
    "To get an understanding of the the cost function, let's plot the cost over a 2-d grid of $\\theta_0$ and $\\theta_1$ values. You will not need to code anything new for this part, but you should understand how the code you have written already is creating these images. \n",
    "\n",
    "The important thing to observe is that the loss function is a convex function that has a clear global minimum that we can find with ease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize grid\n",
    "theta0_vals = np.linspace(-10, 10,100);\n",
    "theta1_vals = np.linspace(-1, 4,100);\n",
    "t0,t1 = np.meshgrid(theta0_vals,theta1_vals)\n",
    "J_vals = np.zeros((t0.shape))\n",
    "\n",
    "#Fill out values\n",
    "for i in range(t0.shape[0]):\n",
    "    for j in range(t0.shape[1]):\n",
    "        t = np.array([t0[i,j],t1[i,j]]).reshape(2,1)\n",
    "        J_vals[i,j] = compute_cost(X,y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d, Axes3D \n",
    "from matplotlib import cm\n",
    "fig=plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.contour(theta0_vals,theta1_vals,np.log(J_vals),15,\n",
    "           cmap=cm.coolwarm)\n",
    "ax1.plot(learned_theta[0,0],learned_theta[1,0],'rx')\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "ax2 = fig.add_subplot(1,2,2,projection='3d')\n",
    "\n",
    "ax2.plot_surface(t0,t1,J_vals,cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Linear regression with multiple variables\n",
    "So you smahsed the linear regression. Dope. Now lets get jazzy with it. \n",
    "\n",
    "In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.<br>\n",
    "The file `ex1data2.txt` contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house.\n",
    "|\n",
    "### 3.1 - Feature normalization\n",
    "After we load the data, by looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.loadtxt('./data/ex1data2.txt',delimiter=',')\n",
    "m=data2.shape[0]\n",
    "X = data2[:,0:-1]\n",
    "y = data2[:,-1].reshape((m,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** complete the function `feature_normalization()` to \n",
    "- subtract the mean of each feature from the dataset\n",
    "- after subtracting the mean, divide by the standard deviation (\"feature scaling\")\n",
    "- be able to do this for datasets of all sizes (arbitrary numbers of features/ training examples)\n",
    "\n",
    "\n",
    "*Hint: numpy has some builtin functions for `mean` and `std`, that allow you to take the measure down a specific axis of the matrix. In our datasets each column will correspond to a feature, and each row will correspond to an example*\n",
    "\n",
    "When normalizing the features, it is important to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters from the model, we often want to predict the prices of houses we have not seen before. Given a new x value (living room area and number of bedrooms), we must first normalize x using the mean and standard deviation that we had previously computed from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalization(data):\n",
    "    '''\n",
    "    Normalizes a set of features within a dataset. \n",
    "    Shifts each feature mean to zero and scales by the standard deviation. \n",
    "    The dataset should be 2-dimensional with each column corresponding to a feature, \n",
    "    and each row corresponding to an example.\n",
    "    \n",
    "    Inputs\n",
    "    ===============\n",
    "    data: complete (m,n) dataset\n",
    "    \n",
    "    Returns\n",
    "    ===============\n",
    "    data: feature-wise normalized (m,n) dataset\n",
    "    mu: feature-wise mean\n",
    "    sigma: feature wise standard deviation ()\n",
    "    '''\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return data,mu,sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 3 examples for training:\\n\",data2[0:3,:])\n",
    "X, mu, sigma = feature_normalization(X)\n",
    "print(\"First 3 example features after normalization:\\n\",X[0:3,:])\n",
    "\n",
    "# Add intercept term\n",
    "X = np.concatenate((np.ones((m,1)),X),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "`First 3 examples for training:\n",
    " [[2.104e+03 3.000e+00 3.999e+05]\n",
    " [1.600e+03 3.000e+00 3.299e+05]\n",
    " [2.400e+03 3.000e+00 3.690e+05]]\n",
    "First 3 example features after normalization:\n",
    " [[ 0.13141542 -0.22609337]\n",
    " [-0.5096407  -0.22609337]\n",
    " [ 0.5079087  -0.22609337]]\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Gradient Descent \n",
    "Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there is one more feature in the matrix X. The hypothesis function and the batch gradient descent update rule remain unchanged.\n",
    "\n",
    "Complete the functions `compute_cost_multi()` and `gradient_descent_multi()` to implement the cost function and gradient descent for linear regression with multiple variables. If your code in the previous part (single variable) already supports multiple variables, you can use it here too. *Make sure your code supports any number of features and is well-vectorized.*\n",
    "\n",
    "The multivariate cost function can be written in the following vectorized form:\n",
    "\\begin{equation*}\n",
    "J(\\theta) = \\frac{1}{2m}(X\\theta - \\bar y)^T (X\\theta-\\bar y)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_multi(X,y,theta): \n",
    "    '''\n",
    "    Computes the cost for linear regression\n",
    "    J = compute_cost(X,y,theta) computes the cost of using theta as the\n",
    "    parameter for linear regression to fit the data points in X and y.\n",
    "    '''\n",
    "    J=0\n",
    "    m=y.shape[0]\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_multi(X,y,theta,alpha,num_iters):\n",
    "    '''\n",
    "    Performs gradient descent to learn theta\n",
    "    \n",
    "    Inputs\n",
    "    ===============\n",
    "    X: input features\n",
    "    y: desired ouput values\n",
    "    alpha: learning rrate\n",
    "    num_iters: number of iteratons for learning\n",
    "    \n",
    "    Returns\n",
    "    ===============\n",
    "    theta: final learned weights\n",
    "    J_history: list of loss over each itteration\n",
    "    '''\n",
    "    m = y.shape[0] #Number of training examples\n",
    "    J_history = np.zeros((num_iters,1))\n",
    "    for i in range(num_iters):\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        J_history[i] = compute_cost(X,y,theta)\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following estimates the price of a three-bedroom house with 1650 square feet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005;\n",
    "num_iters = 600;\n",
    "theta_multi = np.zeros((3,1))\n",
    "learned_theta, J_history = gradient_descent_multi(X,y,theta_multi,alpha,num_iters)\n",
    "x = np.ones((1,3))\n",
    "x[0,1:] = np.array([1985,4])\n",
    "x[0,1:] = x[0,1:] - mu\n",
    "x[0,1:] = x[0,1:]/ sigma\n",
    "\n",
    "price = np.dot(x,learned_theta)\n",
    "# Example plotting\n",
    "fig=plt.figure(figsize=(12,6))\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.plot([a for a in range(num_iters)], J_history,'k.',MarkerSize=5)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Predicted price = ${:12.2f}\".format(price[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Surely there is an easier way....\n",
    "\n",
    "While perhaps the above was a bit daunting at times, it is important to understand the maths behind these aproaches before you use them blind. We also learned how to format our `X` data as a matrix and `y` vector as a matrix. We learned the language of the **hypothesis** function that relates each set of **training features** to an estimate of `y`. \n",
    "\n",
    "Nonetheless, there plenty of packages for implementing machine learning algorithms with just a few lines of code --- if it wasn't so easy, it wouldn't be so popular. \n",
    "\n",
    "The package we will use at this point is called scikit-learn: https://scikit-learn.org/stable/index.html\n",
    "This package implements most shallow machine learning models you will see thrown around in the physical sciences. As you've witnessed in your trial above, always remember:\n",
    "# It's just applied statistics.\n",
    "\n",
    "\n",
    "Now lets reimport out data, and use the scikitlearn version of linear regression on our house data. \n",
    "Calling `linear_model.LinearRegression()` will return a class, that can be used to fit to our X,y data.\n",
    "\n",
    "The methods and attributes of this model class are listed here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "data3 = np.loadtxt('./data/ex1data2.txt',delimiter=',')\n",
    "m=data3.shape[0]\n",
    "X = data3[:,0:-1]\n",
    "y = data3[:,-1].reshape((m,1))\n",
    "X, mu, sigma = feature_normalization(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Call `regr.fit(X,y)` in the cell below. Note that in this case, our X matrix doesn't need a column of ones for the y-intercept term. Scikitlearn takes care of all of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compare the function `predict_price` below to how we made the prediction above. Here we use the `regr.predict()` on a single row of x values; however, we could have predicted on an entirely different set of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(area,n_rooms):\n",
    "    x = np.zeros((1,2))\n",
    "    x[0,:] = np.array([area,n_rooms])\n",
    "    x[0,:] = x[0,:] - mu\n",
    "    x[0,:] = x[0,:]/ sigma\n",
    "    price = regr.predict(x)\n",
    "    \n",
    "    return price\n",
    "print(\"Predicted price = ${:12.2f}\".format(predict_price(1985,4)[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "The cell below will test the functions you produced above, marking failed/ok tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestWeek1(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.id = np.eye(5)\n",
    "        self.theta = np.zeros((2,1))\n",
    "        self.data = np.loadtxt('./data/ex1data1.txt',delimiter=',')\n",
    "        m=self.data.shape[0]\n",
    "        self.X = np.ones((m,2))\n",
    "        self.X[:,1] = self.data[:,0] \n",
    "        self.y = self.data[:,1].reshape((m,1))\n",
    "    def test_identity(self):\n",
    "        self.assertTrue((warmUp().shape[0] == warmUp().shape[1]) and np.allclose(warmUp(), self.id))\n",
    "\n",
    "    def test_compute_cost(self):\n",
    "        self.assertTrue(np.abs((compute_cost(self.X,self.y,self.theta))-32.072)<0.001)\n",
    "        \n",
    "    def test_gradient_descent(self):\n",
    "        lt ,_= gradient_descent(self.X,self.y,self.theta,0.02,2000)\n",
    "        self.assertTrue(np.abs(lt[0,0]--3.8928)<0.0001 and np.abs(lt[1,0]-1.1927)<0.0001)\n",
    "    \n",
    "    def test_gradient_descent_multi(self):\n",
    "        data2 = np.loadtxt('./data/ex1data2.txt',delimiter=',')\n",
    "        m=data2.shape[0]\n",
    "        X = data2[:,0:-1]\n",
    "        y = data2[:,-1].reshape((m,1))\n",
    "        X, mu, sigma = feature_normalization(X)\n",
    "        X = np.concatenate((np.ones((m,1)),X),axis=1)\n",
    "        theta = np.zeros((3,1))\n",
    "        lt,_ = gradient_descent_multi(X,y,theta,0.005,600)\n",
    "        x = np.ones((1,3))\n",
    "        x[0,1:] = np.array([1985,4])\n",
    "        x[0,1:] = x[0,1:] - mu\n",
    "        x[0,1:] = x[0,1:]/ sigma\n",
    "        price = np.dot(x,learned_theta)\n",
    "        self.assertTrue(np.abs(price-331014.11)<0.01)\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
