{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 tutorial: Linear regression\n",
    "\n",
    "### Introduction\n",
    "In this exercise, you will implement linear regression and get to see it work on data. Hopefully you have watched the video lectures associated with this week. **Please keep the function declarations as they are.**\n",
    "\n",
    "To get started with this exercise, you will need to download the Week 1 folder from the github or dropbox.\n",
    "\n",
    "### External files included for this exercise\n",
    " - $\\texttt{ex1data1.txt}$ - Dataset for linear regression with one variable\n",
    " - $\\texttt{ex1data2.txt}$ - Dataset for linear regression with multiple variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Simple numpy function\n",
    "\n",
    "Let's kick things off by making a happy little identity matrix. \n",
    "\n",
    "**Exercise:** Make a function that takes nothing in, and returns a 5x5 identity matrix.\n",
    "\n",
    "*Hint: check the documentation for `np.eye()` and `np.identity()`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def warmUp():\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return A\n",
    "print('Diagonal matrix of ones:\\n', warmUp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "`Diagonal matrix of ones:\n",
    "[[1. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0.]\n",
    " [0. 0. 1. 0. 0.]\n",
    " [0. 0. 0. 1. 0.]\n",
    " [0. 0. 0. 0. 1.]]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Linear regression with one variable\n",
    "In this section you will implement linear regression with a single varaible to predict profits. We have ascertained data from food trucks in various cities that breaks down the profits and populations of those cities. A particularly daft business executive from *Mangia con Maffetone LLC* has approached you with this data and asked for advice on which city to expand to next.\n",
    "\n",
    "The file $\\texttt{ex1data1.txt}$ contains the dataset for our linear regression prob- lem. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss. The file is loaded using the commands in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('./data/ex1data1.txt',delimiter=',') #Loads simple comma separated values\n",
    "X = data[:,0] # Sets all (:) of the first (0) column equal to a new variable X\n",
    "y = data[:,1] # Sets the second column to y\n",
    "m = y.shape[0]\n",
    "print(\"Number of training examples =\",m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Plotting the data\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population). (Many other problems that you will encounter in data science are multi-dimensional and can’t be plotted on a 2D plot.)\n",
    "\n",
    "**Exercise:** Using the X, y variables from above create a plot of the 2D data. Label the y-axis as \"Profit in 10,000s\" and the x-axis \"Population of City in 10,000s\". Use red x's for the markers.<br>\n",
    "*Hint: matplotlib.pyplot has been imported as plt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<img src=\"data/plot.png\" style=\"width: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Gradient Descent\n",
    "In this part, you will fit the linear regression parameters θ to our dataset using gradient descent.\n",
    "\n",
    "#### 2.2.1 - Update Equations\n",
    "The objective of linear regression is to minimize the cost (loss) function\n",
    "\\begin{equation*}\n",
    "J(\\theta) = \\frac{1}{2m}\\sum^{m}_{i=1} (h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "\\end{equation*}\n",
    "where the hypothesis $h_\\theta(x)$ is given by the linear model\n",
    "\\begin{equation*}\n",
    "h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1x_1.\n",
    "\\end{equation*}\n",
    "\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update\n",
    "\\begin{equation*}\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum^{m}_{i=1} (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n",
    "\\end{equation*}\n",
    "simultaneously for all $j$. \n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\\theta)$.\n",
    "\n",
    "#### 2.2.2 - Implementation\n",
    "- Below the data is already set up for linear regression. We add another dimesion (of ones) to our data to accomodate the $\\theta_0$ intercept term. \n",
    "- We also initialize the parameters to 0 and the learning rate `alpha` to 0.01.\n",
    "- Finally, because of array broadcasting rules, life will be easier to debug if we enforce linear algebra. Thus when you make vectors (nx1 arrays), force them into a (n,1) shape instead of the numpy default of (n,). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((m,2)) #Start with a matrix of zeros with 2 columns of m training examples in each row\n",
    "X[:,0] = np.ones(m) # Set the first column to all 1, to multiply against constant parameter\n",
    "X[:,1] = data[:,0] # X data for the first order linear parameter\n",
    "y=y[:,np.newaxis] # This makes y have the shape (97,1) instead of (97,). This is an important implementation distinction. \n",
    "theta = np.zeros((2,1)) # Initialize fitting parameters (weights)\n",
    "\n",
    "iterations = 1500\n",
    "alpha = 0.01 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 - Computing the cost $J(\\theta)$\n",
    "As you perform gradient descent to learn minimize the cost function $J(\\theta)$, it is helpful to monitor the convergence by computing the cost. In this section, you will implement a function to calculate  $J(\\theta)$ so you can check the convergence of your gradient descent implementation.\n",
    "\n",
    "**Exercise:** Complete the function `compute_cost()`, which is a function that computes --- you guessed it --- $J(\\theta)$. <br>\n",
    "*Hint: As you are doing this, remember that $X$ and $y$ are not scalar values, but matricies whose rows represent the examples from the training set. Consider using `np.matmul()` or `np.dot()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X,y,theta):\n",
    "    '''\n",
    "    Computes the cost for linear regression\n",
    "    J = compute_cost(X,y,theta) computes the cost of using theta as the\n",
    "    parameter for linear regression to fit the data points in X and y.\n",
    "    '''\n",
    "    \n",
    "    m = y.shape[0] #Number of training examples\n",
    "    J = 0;\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return J\n",
    "print(\"{:5.2f}\".format(compute_cost(X,y,theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**<br>\n",
    "32.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 - Gradient Descent\n",
    "Next you will implement the gradient descent algorithm. The loop structure has been written for you, and you only need to supply the updates to $\\theta$ within each iteration. \n",
    "\n",
    "As you program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the cost $J(\\theta)$ is parameterized by the vector $\\theta$, and not $X$ and $y$. That is, we minimize the value of $J(\\theta$) by changing the values of the vector $\\theta$, and not by changing $X$ or $y$. \n",
    "\n",
    "**Exercise:** Complete the loop in the function `gradient_descent()` to provide the update for $\\theta$. Please refer to the equations from the video lectures, or above.<br>\n",
    "*Hint: Make sure to vectorize your code using matrix multiplications. If you are unsure of operations, it may help to consider the shape of given arrays and the necessary resultand shapes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,theta,alpha,num_iters):\n",
    "    '''\n",
    "    Performs gradient descent to learn theta\n",
    "    \n",
    "    Inputs\n",
    "    ===============\n",
    "    X: input features\n",
    "    y: desired ouput values\n",
    "    alpha: learning rrate\n",
    "    num_iters: number of iteratons for learning\n",
    "    \n",
    "    Returns\n",
    "    ===============\n",
    "    theta: final learned weights\n",
    "    J_history: list of loss over each itteration\n",
    "    '''\n",
    "    m = y.shape[0] #Number of training examples\n",
    "    J_history = np.zeros((num_iters,1))\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        J_history[i] = compute_cost(X,y,theta)\n",
    "    return theta, J_history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we run the gradient descent function and plot the resultant linear regression fit, as well as the returned loss history. This history allows us to ensure that the loss function is indeed decreasing over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example run\n",
    "learned_theta, J_hist = gradient_descent(X,y,theta,alpha,iterations)\n",
    "# Example plotting\n",
    "fig=plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.plot(X[:,1],y,'rx',MarkerSize=5)\n",
    "plt.xlabel(\"Population of City in 10,000s\")\n",
    "plt.ylabel(\"Profit in $10,000s\")\n",
    "ax1.plot(X[:,1],np.dot(X,learned_theta),'b-')\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.plot([a for a in range(iterations)], J_hist,'k.',MarkerSize=5)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "print('Learned value of theta0:{:7.3f}'.format(learned_theta[0,0]),\n",
    "      '\\nLearned value of theta1:{:7.3f}'.format(learned_theta[1,0]))\n",
    "# Make a prediction\n",
    "predict1 = np.dot(np.array([1,3.5]),learned_theta)\n",
    "print(predict1)\n",
    "print('For population = 35,000, we predict a profit of ${:8.2f}'.format(predict1[0]*10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<img src=\"data/plot2.png\" style=\"width: 600px\"><br>\n",
    "Learned value of theta0: -3.630 <br>\n",
    "Learned value of theta1:  1.166<br>\n",
    "For population = 35,000, we predict a profit of $ 4519.77\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Visualizing $J(\\theta)$\n",
    "\n",
    "To get an understanding of the the cost function, let's plot the cost over a 2-d grid of $\\theta_0$ and $\\theta_1$ values. You will not need to code anything new for this part, but you should understand how the code you have written already is creating these images. \n",
    "\n",
    "The important thing to observe is that the loss function is a convex function that has a clear global minimum that we can find with ease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize grid\n",
    "theta0_vals = np.linspace(-10, 10,100);\n",
    "theta1_vals = np.linspace(-1, 4,100);\n",
    "t0,t1 = np.meshgrid(theta0_vals,theta1_vals)\n",
    "J_vals = np.zeros((t0.shape))\n",
    "\n",
    "#Fill out values\n",
    "for i in range(t0.shape[0]):\n",
    "    for j in range(t0.shape[1]):\n",
    "        t = np.array([t0[i,j],t1[i,j]]).reshape(2,1)\n",
    "        J_vals[i,j] = compute_cost(X,y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d, Axes3D \n",
    "from matplotlib import cm\n",
    "fig=plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.contour(theta0_vals,theta1_vals,np.log(J_vals),15,\n",
    "           cmap=cm.coolwarm)\n",
    "ax1.plot(learned_theta[0,0],learned_theta[1,0],'rx')\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "ax2 = fig.add_subplot(1,2,2,projection='3d')\n",
    "\n",
    "ax2.plot_surface(t0,t1,J_vals,cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Linear regression with multiple variables\n",
    "TBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "The cell below will test the functions you produced above, marking failed/ok tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestWeek1(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.id = np.eye(5)\n",
    "        self.theta = np.zeros((2,1))\n",
    "        data = np.loadtxt('./data/ex1data1.txt',delimiter=',')\n",
    "        m=data.shape[0]\n",
    "        self.X = np.ones((m,2))\n",
    "        self.X[:,1] = data[:,0] \n",
    "        self.y = data[:,1]\n",
    "        self.y = y.reshape((m,1))\n",
    "    def test_identity(self):\n",
    "        self.assertTrue((warmUp().shape[0] == warmUp().shape[1]) and np.allclose(warmUp(), self.id))\n",
    "\n",
    "    def test_compute_cost(self):\n",
    "        self.assertTrue(np.abs((compute_cost(self.X,self.y,self.theta))-32.072)<0.001)\n",
    "        \n",
    "    def test_gradient_descent(self):\n",
    "        lt ,_= gradient_descent(X,y,theta,0.02,2000)\n",
    "        self.assertTrue(np.abs(lt[0,0]--3.8928)<0.0001 and np.abs(lt[1,0]-1.1927)<0.0001)\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
