{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 tutorial: Logistic regression\n",
    "\n",
    "### Introduction\n",
    "In this exercise, you will implement logistic regression and get to see it work on data. After implementing a simple case, we will see how to repeat our work using scikit-learn, then work on more complex cases that require feature mapping and regularization. Hopefully you have watched the video lectures associated with this week. **Please keep the function declarations as they are.**\n",
    "\n",
    "To get started with this exercise, you will need to download the Week 2 folder from the github or dropbox.\n",
    "<br>\n",
    "**Warning: Due to Phil's laziness, the varible names in this sheet get reused across sections. Everything will work from top to bototm; however, if you are having trouble, try restarting your kernel, and running the notebook from the top. **\n",
    "\n",
    "### External files included for this exercise\n",
    " - $\\texttt{ex2data1.txt}$ - Dataset for linear regression with one variable\n",
    " - $\\texttt{ex2data2.txt}$ - Dataset for linear regression with multiple variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Logistic Regression\n",
    "\n",
    "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\n",
    "\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision.\n",
    "\n",
    "Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams. This outline and the framework code in this notebook will guide you through the exercise.\n",
    "\n",
    "### 1.1 - Visualizing the data\n",
    "Before starting to implement any learning algorithm, it is always good to visualize the data if possible. This next cell will load the data and display it on a 2-d plot. The plotting function will display a figure where the axes are the two exam scores, and the positive and negative admission examples are shown with different markers. <br>\n",
    "**Optional exercise:** Take a few moments to change the arguments in the function calls to `plot(...)` to ensure you understand what they each do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('./data/ex2data1.txt',delimiter=',')\n",
    "X = data[:,0:2] \n",
    "m = X.shape[0]\n",
    "one = np.ones((m,1))\n",
    "X = np.concatenate([one,X],axis=1)\n",
    "y = data[:,-1].reshape((m,1))\n",
    "# A fun lil list comprehension to get the relevant row indexs for each case\n",
    "pos = [idx for (idx,val) in enumerate(y) if val==1]\n",
    "neg = [idx for (idx,val) in enumerate(y) if val==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(X[pos,1],X[pos,2],'kx',MarkerSize=8,label='Admitted')\n",
    "plt.plot(X[neg,1],X[neg,2],'ko',MarkerFaceColor='yellow',MarkerSize=8,label='Not admitted')\n",
    "plt.xlim([30,100])\n",
    "plt.ylim([30,100])\n",
    "plt.xlabel(\"Exam 1 score\")\n",
    "plt.ylabel(\"Exam 2 score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Implementation\n",
    "#### 1.2.1 - Sigmoid function\n",
    "Before we start with the actual cost function, let's recall that the logistic regression hypothesis is defined as:\n",
    "\\begin{equation*}\n",
    "h_\\theta = g(x\\theta)\n",
    "\\end{equation*}\n",
    "where $g(z)$ is the sigmoid function, \n",
    "\\begin{equation*}\n",
    "g(z) = \\frac{1}{1+\\mathrm{e}^{-z}}\n",
    "\\end{equation*}\n",
    "$\\theta$ is a (n,1) matrix, and $x$ is a single example (1,n).\n",
    "<br>\n",
    "Your first step is to implement the sigmoid function below. When you are finished, try testing a few values by calling `sigmoid(x)`. For large values of $x$, the sigmoid should be close to 1, while for large negative values, the sigmoid should be clost to 0. Evaluating `sigmoid(0)` should give exactly 0.5. \n",
    "<br>\n",
    "**Exercise: Finish the function below, such that it will calculate the sigmoid value elementwise for vectors and matricies **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Computes the sigmoid function elementwise\n",
    "    \n",
    "    Input\n",
    "    ==============\n",
    "    x : scalar, vector or martrix of values\n",
    "    \n",
    "    Returns\n",
    "    ==============\n",
    "    g : elementwise sigmoid function. Same shape as x.\n",
    "    '''\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return g\n",
    "\n",
    "print(sigmoid(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 - Cost function and gradient\n",
    "Similar to what we did with linear regression, we will now implement the cost function and gradient for logistic regression. Recall that the cost function for logistic regression is the cross entropy\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\frac{1}{m}\\sum^m_{i=1} \\left[\n",
    "-y^{(i)} \\log(h_\\theta(x^{(i)})) - (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))\n",
    "\\right]\n",
    "\\end{equation}\n",
    "and the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{\\mathrm{th}}$ element (for $j=-,1\\dots,n$) is defined as follows:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\theta)}{\\partial\\theta_j} = \\frac{1}{m} \\sum^m_{i=1} (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\n",
    "\\end{equation}\n",
    "\n",
    "Here the sums are operating down the $X$ matrix across all trainging examples, and the $j$ refers to particular columns of *features*. Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$. \n",
    "\n",
    "**Exercise: Implement the function `compute_cost()` to return both the loss function J, and the gradient.**\n",
    "<br>\n",
    "*Hint: A vectorized way of calculating the sum of the elements of a product is an inner product. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(theta,X,y):\n",
    "    '''\n",
    "    Computs the cost of using theta as the parameters for logistic regression\n",
    "    and the gradient of the cost with respect ot the parameters. \n",
    "    \n",
    "    Input\n",
    "    ==============\n",
    "    theta : (n,1) vector of parameters\n",
    "    X : (m,n) matrix of training data\n",
    "    y : (m,1) vector of ground truth data\n",
    "    \n",
    "    Returns\n",
    "    ==============\n",
    "    J : cost\n",
    "    grad : (n,1) vector of gradients\n",
    "    '''\n",
    "    \n",
    "    # Initialize useful parameters\n",
    "    m = X. shape[0]\n",
    "    # You will need to return the following correctly\n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return J,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=np.zeros((X.shape[1],1))\n",
    "J,grad = compute_cost(theta,X,y)\n",
    "print(\"Initial cost = {:6.4f}\".format(J[0,0]))\n",
    "print(\"Initial gradiaent =\\n\",grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**<br>\n",
    "`Initial cost = 0.6931\n",
    " Initial gradiaent =\n",
    " [[ -0.1       ]\n",
    " [-12.00921659]\n",
    " [-11.26284221]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 - Gradient descent\n",
    "\n",
    "We can implement the gradient descent algorithm exactly as we did in Linear regression. Below however, the cost computation includes the gradient calculation, so the update of theta is much simpler. \n",
    "\n",
    "I've left a cell after the gradient descent cell to show how the loss changes over time, and replot the data with your derived linear decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,theta,alpha=0.00001,num_iters=1500):\n",
    "    '''\n",
    "    Performs gradient descent to learn theta\n",
    "    \n",
    "    Inputs\n",
    "    ===============\n",
    "    X: input features\n",
    "    y: desired ouput values\n",
    "    alpha: learning rrate\n",
    "    num_iters: number of iteratons for learning\n",
    "    \n",
    "    Returns\n",
    "    ===============\n",
    "    theta: final learned weights\n",
    "    J_history: list of loss over each itteration\n",
    "    '''\n",
    "    m = y.shape[0] #Number of training examples\n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate cost and gradient\n",
    "        J, grad = compute_cost(theta,X,y)\n",
    "        J_history.append(J[0,0])\n",
    "        # Update theta\n",
    "        theta = theta - alpha * grad\n",
    "\n",
    "    return theta, J_history\n",
    "\n",
    "initial_theta=np.array([[-13],[0.2],[0.2]])\n",
    "num_iters=1500\n",
    "learned_theta, J_hist = gradient_descent(X,y,initial_theta,num_iters=num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding decision boundary (this uses an analytical function derived from the 0.5 condition)\n",
    "plot_x = [np.min(X[:,1]),np.max(X[:,1])]\n",
    "plot_y = [-1/learned_theta[2,0]*(learned_theta[1,0]*x + learned_theta[0,0]) for x in plot_x]\n",
    "                \n",
    "fig1=plt.figure(figsize=(16,6))\n",
    "ax1 = fig1.add_subplot(1,2,1)\n",
    "ax1.plot(X[pos,1],X[pos,2],'kx',MarkerSize=8,label='Admitted')\n",
    "ax1.plot(X[neg,1],X[neg,2],'ko',MarkerFaceColor='yellow',MarkerSize=8,label='Not admitted')\n",
    "ax1.plot(plot_x,plot_y,'b-',label='Decision boundary')\n",
    "ax1.legend()\n",
    "ax1.set_xlim([30,100])\n",
    "ax1.set_ylim([30,100])\n",
    "ax1.set_xlabel('Exam 1 score')\n",
    "ax1.set_ylabel('Exam 2 score')\n",
    "\n",
    "ax2 = fig1.add_subplot(1,2,2)\n",
    "ax2.plot([a for a in range(1500)], J_hist,'k.',MarkerSize=5)\n",
    "ax2.set_xlabel(\"Number of iterations\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 - Evaluating your function\n",
    "After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should expect to see an admission probability of **?**.\n",
    "Another way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. The predict function will produce “1” or “0” predictions given a dataset and a learned parameter vector $\\theta$. Then there is a script below to calculate the training accuracy of your model. \n",
    "<br>\n",
    "**Exercise: Comlete the function `predict()` below, to make a predicted y value of 0 or 1.**<br>\n",
    "*Hint: use the round function or a logical statement (int(True)=1, int(False)=0).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta,X):\n",
    "    '''\n",
    "    Makes a prediction using a learned set of values for theta.\n",
    "    \n",
    "    Inputs\n",
    "    ===============\n",
    "    theta : (n,1) vector of parameters\n",
    "    X : (m,n) matrix of training data\n",
    "    \n",
    "    Returns\n",
    "    ===============\n",
    "    p: (m,1) vector of calclated prediction values\n",
    "    '''\n",
    "    m = X. shape[0]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return p\n",
    "\n",
    "p = predict(learned_theta,X)\n",
    "acc = np.mean(p==y)*100\n",
    "print(\"Training accuracy: \" ,acc)\n",
    "print(\"Expected accuracy (approx): 89.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 - Blackbox\n",
    "Excellent work! Now lets take a look at the sklearn implementation. First we'll reimport our data with the intercept column, and use the scikitlearn version of logistic regression on our exam data. \n",
    "Calling `linear_model.LogisticRegression()` will return a class, that can be used to fit to our X,y data.\n",
    "\n",
    "The methods and attributes of this model class are listed here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "In breif:\n",
    "- First we initialize the class\n",
    "- Second we call the .fit() method on the data\n",
    "- The prediction and scoring is also taken care of in the available methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Load data: Notice how the shape of y is now (m,) not (m,1)\n",
    "data = np.loadtxt('./data/ex2data1.txt',delimiter=',')\n",
    "X = data[:,0:2] \n",
    "m = X.shape[0]\n",
    "one = np.ones((m,1))\n",
    "X = np.concatenate([one,X],axis=1)\n",
    "m = X.shape[0]\n",
    "y = data[:,-1]\n",
    "# Execute regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X,y)\n",
    "print(\"Training accuracy: \",log_reg.score(X,y))\n",
    "print(\"Even better than before!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a bit of complicated graphworks. While before we plotted a decision boundary at P=0.5, this shows a contour plot of the probability of admisison given a set of scores. Overlayed are the points colored according to their binary ground truth. The decision boundary is given by the P=0.5 line in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[30:100:0.1, 30:100:0.1]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "one = np.ones((grid.shape[0],1))\n",
    "grid=np.concatenate((one,grid),axis=1)\n",
    "probs = log_reg.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                      vmin=0, vmax=1)\n",
    "ax_c = f.colorbar(contour)\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "ax.contour(xx, yy, probs, levels=[0.5],vmin=0, vmax=1,colors='green')\n",
    "ax.scatter(X[:,1], X[:, 2], c=y[:], s=50,\n",
    "           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(30, 100), ylim=(30, 100),\n",
    "       xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Regularized logistic regression\n",
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n",
    "\n",
    "### 2.1 - Visualizing the data\n",
    "**The following import and visualization does not include a first column of ones as an intercept term.**<br>\n",
    "Examining the data using an adaptation of the plotting function from above, this shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight-forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('./data/ex2data2.txt',delimiter=',')\n",
    "X1 = data[:,0]\n",
    "X2 = data[:,1]\n",
    "m = X1.shape[0]\n",
    "y = data[:,-1]\n",
    "pos = [idx for (idx,val) in enumerate(y) if val==1]\n",
    "neg = [idx for (idx,val) in enumerate(y) if val==0]\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(X1[pos],X2[pos],'kx',MarkerSize=8,label='Accepted')\n",
    "plt.plot(X1[neg],X2[neg],'ko',MarkerFaceColor='yellow',MarkerSize=8,label='Rejected')\n",
    "plt.xlabel(\"Microchip test 1\")\n",
    "plt.ylabel(\"Microchip test 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Feature mapping\n",
    "One way to fit the data better is to create more features from each datapoint. In the below function `map_feature()`, we can map the features into all polynomial terms of $x_1$ and $x_2$ up to the $n^\\mathrm{th}$ power (we'll use $n=6$).\n",
    "\\begin{equation}\n",
    "\\mathrm{map\\_feature}(x) = \\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_1^2\\\\\n",
    "x_1x_2\\\\\n",
    "x_2^2 \\\\\n",
    "x_1^3\\\\\n",
    "\\vdots\\\\\n",
    "x_1x_2^5\\\\\n",
    "x_2^6\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 27-dimensional vector.  A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot. *Note that we've excluded the 0$^{th}$ order term, as this intercept will come in separately.*<br>\n",
    "\n",
    "\n",
    "\n",
    "While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature(X1,X2,degree=6):\n",
    "    '''\n",
    "    Returns a matrix of feature vectors of the nth degree polynomial terms\n",
    "    of two variables. Does not include constant intercept term. \n",
    "    \n",
    "    Inputs\n",
    "    ===============\n",
    "    X1 : (m,1) vector of feature 1\n",
    "    X2 : (m,1) vector of feature 2\n",
    "    degree : degree of polynomial features\n",
    "    \n",
    "    Returns\n",
    "    ===============\n",
    "    out: (m,n) new matrix of training data\n",
    "    '''\n",
    "    new_size = (degree+2)*(degree+1) //2-1\n",
    "    out = np.zeros((X1.shape[0],new_size))\n",
    "    idx=0\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(0,i+1):\n",
    "            out[:,idx] = X1**(i-j) * X2**j\n",
    "            idx+=1\n",
    "    return out\n",
    "X = map_feature(X1,X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Cost function and gradient\n",
    "**Regularization** introduces a penalty term for specific weights becoming too large. This effectively reduces the learning algorithm's ability to over-fit a function, and favors a smooth decision boundary. Below, I will reiterate the functional form of the loss and gradient; however, we will opt for convenience and use scikit-learn for implementation. \n",
    "\n",
    "The regularized cost funciton in logistic regression is the cross-entropy loss with an added term:\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\frac{1}{m}\\sum^m_{i=1} \\left[\n",
    "-y^{(i)} \\log(h_\\theta(x^{(i)})) - (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))\n",
    "\\right]\n",
    "+ \\frac{\\lambda}{2m} \\sum^n_{j=1}\\theta_j^2\n",
    "\\end{equation}\n",
    "We do not regularize the intercept term, which is why it was excluded from our implementation of `map_feature()`. The gradient of the cost function is a vector where the $j^{\\mathrm{th}}$ element is defined as:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\theta)}{\\partial\\theta_j} = \\left(\\frac{1}{m} \\sum^m_{i=1} (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\\right) + \\frac{\\lambda}{2m}\\theta_j\n",
    "\\end{equation}\n",
    "\n",
    "#### 2.3.1 - Implementation with sklearn\n",
    "You should feel confident that you could program this gradient descent on logistic regression with regularization. For the sake of breivity, we will use a handy machine learning library to do this for us. <br>\n",
    "**Excercise :**\n",
    "- Following along from the Exam Scores example, create a call to the class `LogistiRegression()`.\n",
    "- In the initializer, specify the arguments `fit_intercept=True`, `C=1`. C is the inverse of $\\lambda$: smaller values specify stronger regularization.\n",
    "    - A sensible line would look something like `log_reg = LogisticRegression(fit_intercept=True, C=1)\n",
    "- Next call the `.fit()` method on our variables `X` and `y`. \n",
    "- Print the training accuracy using the `.score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Understanding our results. \n",
    "Below, I've given you another cell that creates a lovely graph to examine the results of our algorithm. \n",
    "<br>\n",
    "*Remember, all the machine learns is a function of probability with resepct to our high dimensional input. Anything with a probability less than 0.5 is a 'negative' example, and above such is a positive example.* <br>\n",
    "**Exercise: Lets have a play with regularization.** The implementation of regularization in `sklearn.linear_model.LogisticRegression` uses a value of `C` that is the inverse of `lambda` in our equations above. Run the above and below cells with the following values for `C`:\n",
    "- C = 1 | $\\lambda$ = 1 : Appropriate regularization\n",
    "- C = 0.01 | $\\lambda$ = 100 : Too much regularization, *i.e.* poor fit. \n",
    "- C = 1e5 | $\\lambda$ = 1e-5 : No regularization,  *i.e.* overfitting. \n",
    "\n",
    "Explore different values of regularization. In practice, we would train only on a portion of our data, and use a second portion to test the predicability of our model. Models that are overfit may have a high accuracy; however, they have limited predictive capacity and are thus useless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-1:1.2:0.01, -1:1.2:0.01]\n",
    "grid = map_feature(xx.ravel(), yy.ravel())\n",
    "\n",
    "probs = log_reg.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                      vmin=0, vmax=1)\n",
    "ax_c = f.colorbar(contour)\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "ax.contour(xx, yy, probs, levels=[0.5],vmin=0, vmax=1,colors='green')\n",
    "\n",
    "ax.scatter(X[:,0], X[:, 1], c=y[:], s=50,\n",
    "           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(-1, 1.2), ylim=(-1, 1.2),\n",
    "       xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
