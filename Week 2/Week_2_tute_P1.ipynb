{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 tutorial: Logistic regression\n",
    "\n",
    "### Introduction\n",
    "In this exercise, you will implement logistic regression and get to see it work on data. After implementing a simple case, we will see how to repeat our work using scikit-learn, then work on more complex cases that require feature mapping and regularization. Hopefully you have watched the video lectures associated with this week. **Please keep the function declarations as they are.**\n",
    "\n",
    "To get started with this exercise, you will need to download the Week 2 folder from the github or dropbox.\n",
    "<br>\n",
    "\n",
    "### External files included for this exercise\n",
    " - $\\texttt{ex2data1.txt}$ - Dataset for linear regression with one variable\n",
    " - $\\texttt{ex2data2.txt}$ - Dataset for linear regression with multiple variables\n",
    " - Week_2_tute_P1 - Part one of this week's exercise.\n",
    " - Week_2_tute_P2 - Part two of this week's exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Logistic Regression\n",
    "\n",
    "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\n",
    "\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision.\n",
    "\n",
    "Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams. This outline and the framework code in this notebook will guide you through the exercise.\n",
    "\n",
    "### 1.1 - Visualizing the data\n",
    "Before starting to implement any learning algorithm, it is always good to visualize the data if possible. This next cell will load the data and display it on a 2-d plot. The plotting function will display a figure where the axes are the two exam scores, and the positive and negative admission examples are shown with different markers. <br>\n",
    "**Optional exercise:** Take a few moments to change the arguments in the function calls to `plot(...)` to ensure you understand what they each do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('./data/ex2data1.txt',delimiter=',')\n",
    "X = data[:,0:2] \n",
    "m = X.shape[0]\n",
    "one = np.ones((m,1))\n",
    "X = np.concatenate([one,X],axis=1)\n",
    "y = data[:,-1].reshape((m,1))\n",
    "# A fun lil list comprehension to get the relevant row indexs for each case\n",
    "pos = [idx for (idx,val) in enumerate(y) if val==1]\n",
    "neg = [idx for (idx,val) in enumerate(y) if val==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot(X[pos,1],X[pos,2],'kx',MarkerSize=8,label='Admitted')\n",
    "plt.plot(X[neg,1],X[neg,2],'ko',MarkerFaceColor='yellow',MarkerSize=8,label='Not admitted')\n",
    "plt.xlim([30,100])\n",
    "plt.ylim([30,100])\n",
    "plt.xlabel(\"Exam 1 score\")\n",
    "plt.ylabel(\"Exam 2 score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Implementation\n",
    "#### 1.2.1 - Sigmoid function\n",
    "Before we start with the actual cost function, let's recall that the logistic regression hypothesis is defined as:\n",
    "\\begin{equation*}\n",
    "h_\\theta = g(x\\theta)\n",
    "\\end{equation*}\n",
    "where $g(z)$ is the sigmoid function, \n",
    "\\begin{equation*}\n",
    "g(z) = \\frac{1}{1+\\mathrm{e}^{-z}}\n",
    "\\end{equation*}\n",
    "$\\theta$ is a (n,1) matrix, and $x$ is a single example (1,n).\n",
    "<br>\n",
    "Your first step is to implement the sigmoid function below. When you are finished, try testing a few values by calling `sigmoid(x)`. For large values of $x$, the sigmoid should be close to 1, while for large negative values, the sigmoid should be clost to 0. Evaluating `sigmoid(0)` should give exactly 0.5. \n",
    "<br>\n",
    "**Exercise: Finish the function below, such that it will calculate the sigmoid value elementwise for vectors and matricies **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Computes the sigmoid function elementwise\n",
    "    \n",
    "    Input\n",
    "    ==============\n",
    "    x : scalar, vector or martrix of values\n",
    "    \n",
    "    Returns\n",
    "    ==============\n",
    "    g : elementwise sigmoid function. Same shape as x.\n",
    "    '''\n",
    "    ### START CODE HERE\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return g\n",
    "\n",
    "print(sigmoid(+10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 - Cost function and gradient\n",
    "Similar to what we did with linear regression, we will now implement the cost function and gradient for logistic regression. Recall that the cost function for logistic regression is the cross entropy\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\frac{1}{m}\\sum^m_{i=1} \\left[\n",
    "-y^{(i)} \\log(h_\\theta(x^{(i)})) - (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))\n",
    "\\right]\n",
    "\\end{equation}\n",
    "and the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{\\mathrm{th}}$ element (for $j=-,1\\dots,n$) is defined as follows:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\theta)}{\\partial\\theta_j} = \\frac{1}{m} \\sum^m_{i=1} (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\n",
    "\\end{equation}\n",
    "\n",
    "Here the sums are operating down the $X$ matrix across all trainging examples, and the $j$ refers to particular columns of *features*. Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$. \n",
    "\n",
    "**Exercise: Implement the function `compute_cost()` to return both the loss function J, and the gradient.**\n",
    "<br>\n",
    "*Hint: A vectorized way of calculating the sum of the elements of a product is an inner product. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(theta,X,y):\n",
    "    '''\n",
    "    Computs the cost of using theta as the parameters for logistic regression\n",
    "    and the gradient of the cost with respect ot the parameters. \n",
    "    \n",
    "    Input\n",
    "    ==============\n",
    "    theta : (n,1) vector of parameters\n",
    "    X : (m,n) matrix of training data\n",
    "    y : (m,1) vector of ground truth data\n",
    "    \n",
    "    Returns\n",
    "    ==============\n",
    "    J : cost\n",
    "    grad : (n,1) vector of gradients\n",
    "    '''\n",
    "    \n",
    "    # Initialize useful parameters\n",
    "    m = X. shape[0]\n",
    "    # You will need to return the following correctly\n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return J,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=np.zeros((X.shape[1],1))\n",
    "J,grad = compute_cost(theta,X,y)\n",
    "print(\"Initial cost = {:6.4f}\".format(J[0,0]))\n",
    "print(\"Initial gradiaent =\\n\",grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**<br>\n",
    "`Initial cost = 0.6931\n",
    " Initial gradiaent =\n",
    " [[ -0.1       ]\n",
    " [-12.00921659]\n",
    " [-11.26284221]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 - Gradient descent\n",
    "\n",
    "We can implement the gradient descent algorithm exactly as we did in Linear regression. Below however, the cost computation includes the gradient calculation, so the update of theta is much simpler. \n",
    "\n",
    "I've left a cell after the gradient descent cell to show how the loss changes over time, and replot the data with your derived linear decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,theta,alpha=0.00001,num_iters=1500):\n",
    "    '''\n",
    "    Performs gradient descent to learn theta\n",
    "    \n",
    "    Inputs\n",
    "    ===============\n",
    "    X: input features\n",
    "    y: desired ouput values\n",
    "    alpha: learning rrate\n",
    "    num_iters: number of iteratons for learning\n",
    "    \n",
    "    Returns\n",
    "    ===============\n",
    "    theta: final learned weights\n",
    "    J_history: list of loss over each itteration\n",
    "    '''\n",
    "    m = y.shape[0] #Number of training examples\n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate cost and gradient\n",
    "        J, grad = compute_cost(theta,X,y)\n",
    "        J_history.append(J[0,0])\n",
    "        # Update theta\n",
    "        theta = theta - alpha * grad\n",
    "\n",
    "    return theta, J_history\n",
    "\n",
    "initial_theta=np.array([[-13],[0.2],[0.2]])\n",
    "num_iters=1500\n",
    "learned_theta, J_hist = gradient_descent(X,y,initial_theta,num_iters=num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding decision boundary (this uses an analytical function derived from the 0.5 condition)\n",
    "plot_x = [np.min(X[:,1]),np.max(X[:,1])]\n",
    "plot_y = [-1/learned_theta[2,0]*(learned_theta[1,0]*x + learned_theta[0,0]) for x in plot_x]\n",
    "                \n",
    "fig1=plt.figure(figsize=(16,6))\n",
    "ax1 = fig1.add_subplot(1,2,1)\n",
    "ax1.plot(X[pos,1],X[pos,2],'kx',MarkerSize=8,label='Admitted')\n",
    "ax1.plot(X[neg,1],X[neg,2],'ko',MarkerFaceColor='yellow',MarkerSize=8,label='Not admitted')\n",
    "ax1.plot(plot_x,plot_y,'b-',label='Decision boundary')\n",
    "ax1.legend()\n",
    "ax1.set_xlim([30,100])\n",
    "ax1.set_ylim([30,100])\n",
    "ax1.set_xlabel('Exam 1 score')\n",
    "ax1.set_ylabel('Exam 2 score')\n",
    "\n",
    "ax2 = fig1.add_subplot(1,2,2)\n",
    "ax2.plot([a for a in range(1500)], J_hist,'k.',MarkerSize=5)\n",
    "ax2.set_xlabel(\"Number of iterations\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 - Evaluating your function\n",
    "After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should expect to see an admission probability of **?**.\n",
    "Another way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. The predict function will produce “1” or “0” predictions given a dataset and a learned parameter vector $\\theta$. Then there is a script below to calculate the training accuracy of your model. \n",
    "<br>\n",
    "**Exercise: Comlete the function `predict()` below, to make a predicted y value of 0 or 1.**<br>\n",
    "*Hint: use the round function or a logical statement (int(True)=1, int(False)=0).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta,X):\n",
    "    '''\n",
    "    Makes a prediction using a learned set of values for theta.\n",
    "    \n",
    "    Inputs\n",
    "    ===============\n",
    "    theta : (n,1) vector of parameters\n",
    "    X : (m,n) matrix of training data\n",
    "    \n",
    "    Returns\n",
    "    ===============\n",
    "    p: (m,1) vector of calclated prediction values\n",
    "    '''\n",
    "    m = X. shape[0]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return p\n",
    "\n",
    "p = predict(learned_theta,X)\n",
    "acc = np.mean(p==y)*100\n",
    "print(\"Training accuracy: \" ,acc)\n",
    "print(\"Expected accuracy (approx): 89.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 - Blackbox\n",
    "Excellent work! Now lets take a look at the sklearn implementation. First we'll reimport our data with the intercept column, and use the scikitlearn version of logistic regression on our exam data. \n",
    "Calling `linear_model.LogisticRegression()` will return a class, that can be used to fit to our X,y data.\n",
    "\n",
    "The methods and attributes of this model class are listed here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "In breif:\n",
    "- First we initialize the class\n",
    "- Second we call the .fit() method on the data\n",
    "- The prediction and scoring is also taken care of in the available methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Load data: Notice how the shape of y is now (m,) not (m,1)\n",
    "data = np.loadtxt('./data/ex2data1.txt',delimiter=',')\n",
    "X = data[:,0:2] \n",
    "m = X.shape[0]\n",
    "one = np.ones((m,1))\n",
    "X = np.concatenate([one,X],axis=1)\n",
    "m = X.shape[0]\n",
    "y = data[:,-1]\n",
    "# Execute regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X,y)\n",
    "print(\"Training accuracy: \",log_reg.score(X,y))\n",
    "print(\"Even better than before!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a bit of complicated graphworks. While before we plotted a decision boundary at P=0.5, this shows a contour plot of the probability of admisison given a set of scores. Overlayed are the points colored according to their binary ground truth. The decision boundary is given by the P=0.5 line in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[30:100:0.1, 30:100:0.1]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "one = np.ones((grid.shape[0],1))\n",
    "grid=np.concatenate((one,grid),axis=1)\n",
    "probs = log_reg.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                      vmin=0, vmax=1)\n",
    "ax_c = f.colorbar(contour)\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "ax.contour(xx, yy, probs, levels=[0.5],vmin=0, vmax=1,colors='green')\n",
    "ax.scatter(X[:,1], X[:, 2], c=y[:], s=50,\n",
    "           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(30, 100), ylim=(30, 100),\n",
    "       xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
